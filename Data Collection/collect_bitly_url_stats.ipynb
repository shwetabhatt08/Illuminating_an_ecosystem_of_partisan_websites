{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import glob\n",
    "from os.path import expanduser as ospath\n",
    "from urlparse import urlparse\n",
    "import os\n",
    "import requests\n",
    "from requests_toolbelt.utils import dump\n",
    "import unicodedata\n",
    "import facebook\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "dir_name = 'data/statuses'\n",
    "json_pattern = os.path.join(dir_name,'*.csv')\n",
    "file_list = glob.glob(json_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separating active pages from deactivated ones\n",
    "map_df = pandas.read_csv('data/fb_pages_mapping.csv')\n",
    "active_page_files = []\n",
    "active_ids = list(map_df['id'])\n",
    "total_post_count = 0\n",
    "for each_file in file_list:\n",
    "    page_id = each_file.split('/')[2].split('_')[0]\n",
    "    if int(page_id) in active_ids:\n",
    "        active_page_files.append(each_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total count of posts\n",
    "for each_file in active_page_files:\n",
    "    each_df = pandas.read_csv(each_file)\n",
    "    total_post_count += len(each_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out how many status types are present in the data\n",
    "links_type_dict = {}\n",
    "for each_file in active_page_files:\n",
    "    links_counter = {}\n",
    "    file_df = pandas.read_csv(each_file)\n",
    "    links_counter = Counter(file_df['status_type'])\n",
    "    page_id = each_file.split('/')[2].split('_')[0]\n",
    "    page_name = list(map_df[map_df['id'] == int(page_id)]['page_name'])[0]\n",
    "    links_type_dict[page_name] = links_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary indicating status type and total post count in each category\n",
    "status_type_dict = {}\n",
    "keys = ['event','link','music','note','offer','photo','status','video',]\n",
    "for key, val in links_type_dict.items():\n",
    "    for each_key in keys:\n",
    "        if each_key in val.keys():\n",
    "            if status_type_dict.has_key(each_key) == False:\n",
    "                status_type_dict[each_key] = val[each_key]\n",
    "            else:\n",
    "                status_type_dict[each_key] += val[each_key]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'event': 1511,\n",
       " 'link': 3501965,\n",
       " 'music': 88,\n",
       " 'note': 50,\n",
       " 'offer': 19,\n",
       " 'photo': 458183,\n",
       " 'status': 30621,\n",
       " 'video': 154108}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.4582079631718"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total percentage of links\n",
    "total = status_type_dict['event'] + status_type_dict['link'] + status_type_dict['photo'] + status_type_dict['video']+ status_type_dict['status']\n",
    "float(status_type_dict['link']*100)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urlclean\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import pandas\n",
    "import pandas as pd\n",
    "from urlparse import urlparse\n",
    "import json\n",
    "# list of url shorteners\n",
    "url_shorteners = ['bit.ly','goo.gl','tinyurl.com','is.gd','cli.gs','pic.gd','tweetphoto','DwarfURL.com','ow.ly','yfrog.com','migre.me','ff.im','tiny.cc','url4.eu','tr.im','twit.ac','su.pr','twurl.nl','snipurl.com','BudURL.com','short.to','ping.fm','Digg.com','post.ly','Just.as','.tk','bkite.com','snipr.com','flic.kr','loopt.us','doiop.com','twitthis.com','htxt.it','AltURL.com','RedirX.com','DigBig.com','short.ie','u.mavrev.com','kl.am','wp.me','u.nu','rubyurl.com','om.ly','linkbee.com','Yep.it','posted.at','xrl.us','metamark.net','sn.im','hurl.ws','eepurl.com','idek.net','urlpire.com','chilp.it','moourl.com','snurl.com','xr.com','lin.cr','EasyURI.com','zz.gd','ur1.ca','URL.ie','adjix.com','twurl.cc','s7y.us','shrinkify','EasyURL.net','atu.ca','sp2.ro','Profile.to','ub0.cc','minurl.fr','cort.as','fire.to','2tu.us','twiturl.de','to.ly','BurnURL.com','nn.nf','clck.ru','notlong.com','thrdl.es','spedr.com','vl.am','miniurl.com','virl.com','PiURL.com','1url.com','gri.ms','tr.my','Sharein.com','urlzen.com','fon.gs','Shrinkify.com','ri.ms','b23.ru','Fly2.ws','xrl.in','Fhurl.com','wipi.es','korta.nu','shortna.me','fa.b','WapURL.co.uk','urlcut.com','6url.com','abbrr.com','SimURL.com','klck.me','x.se','2big.at','url.co.uk','ewerl.com','inreply.to','TightURL.com','a.gg','tinytw.it','zi.pe','riz.gd','hex.io','fwd4.me','bacn.me','shrt.st','ln-s.ru','tiny.pl','o-x.fr','StartURL.com','jijr.com','shorl.com','icanhaz.com','updating.me','kissa.be','hellotxt.com','pnt.me','nsfw.in','xurl.jp','yweb.com','urlkiss.com','QLNK.net','w3t.org','lt.tl','twirl.at','zipmyurl.com','urlot.com','a.nf','hurl.me','URLHawk.com','Tnij.org','4url.cc','firsturl.de','Hurl.it','sturly.com','shrinkster.com','ln-s.net','go2cut.com','liip.to','shw.me','XeeURL.com','liltext.com','lnk.gd','xzb.cc','linkbun.ch','href.in','urlbrief.com','2ya.com','safe.mn','shrunkin.com','bloat.me','krunchd.com','minilien.com','ShortLinks.co.uk','qicute.com','rb6.me','urlx.ie','pd.am','go2.me','tinyarro.ws','tinyvid.io','lurl.no','ru.ly','lru.jp','rickroll.it','togoto.us','ClickMeter.com','hugeurl.com','tinyuri.ca','shrten.com','shorturl.com','Quip-Art.com','urlao.com','a2a.me','tcrn.ch','goshrink.com','DecentURL.com','decenturl.com','zi.ma','1link.in','sharetabs.com','shoturl.us','fff.to','hover.com','lnk.in','jmp2.net','dy.fi','urlcover.com','2pl.us','tweetburner.com','u6e.de','xaddr.com','gl.am','dfl8.me','go.9nl.com','gurl.es','C-O.IN','TraceURL.com','liurl.cn','MyURL.in','urlenco.de','ne1.net','buk.me','rsmonkey.com','cuturl.com','turo.us','sqrl.it','iterasi.net','tiny123.com','EsyURL.com','urlx.org','IsCool.net','twitterpan.com','GoWat.ch','poprl.com','njx.me']\n",
    "json_data = open('fb_status_link.json').read()\n",
    "fb_data = json.loads(json_data)\n",
    "inactive_ids = ['1205242106174446', '125577181211429','1349936971683802', '1460363040662703','1488768428099602', '156351959061001', '167870080364375','1744605802516147', '1758142167775100', '1858711167741099', '299402180407111', '324090427983707', '428978587288437', '503743826481257', '54701331336','690344821147429', '862847370480630', '89742953276', '938667992913151', '942564539125675']\n",
    "active_fb_pages = []\n",
    "short_url_count_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for each_file in file_list:\n",
    "    page_id = each_file.split(\"/\")[2].split(\"_\")[0]\n",
    "    if page_id not in inactive_ids:\n",
    "        active_fb_pages.append(each_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a dictionary of short urls for each Fb page\n",
    "import json\n",
    "short_url_dict = {}\n",
    "json_data = open('fb_status_link.json').read()\n",
    "fb_data = json.loads(json_data)\n",
    "for key, val in fb_data.items():\n",
    "    short_url_dict[key] = {}\n",
    "    for each_key, each_val in val['status_link'].items():\n",
    "        if each_key in url_shorteners:\n",
    "            short_url_dict[key][each_key] = each_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_url_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pasges that do not contain any short urls\n",
    "non_bitly_pages = []\n",
    "for key, val in short_url_dict.items():\n",
    "    if val == {}:\n",
    "        non_bitly_pages.append(key)\n",
    "        short_url_dict.pop(key)\n",
    "len(non_bitly_pages)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shwetabhatt/anaconda/lib/python2.7/site-packages/pandas/core/algorithms.py:399: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  f = lambda x, y: htable.ismember_object(x, values)\n"
     ]
    }
   ],
   "source": [
    "map_df = pandas.read_csv('data/fb_pages_mapping.csv')\n",
    "active_ids = list(map_df['id'])\n",
    "active_names = list(map_df['page_name'])\n",
    "\n",
    "long_url_page_ids = list(map_df[map_df['page_name'].isin(non_bitly_pages)]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of page ids of Fb pages containing short urls as statuses\n",
    "short_url_page_ids = []\n",
    "for each_id in list(map_df['id']):\n",
    "    if each_id not in long_url_page_ids:\n",
    "        short_url_page_ids.append(each_id)\n",
    "len(short_url_page_ids)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the total count of short urls across each Fb page\n",
    "short_url_count_dict = {}\n",
    "\n",
    "for key, val in short_url_dict.items():\n",
    "    total_short_urls = 0\n",
    "    for each_key, each_val in val.items():\n",
    "        total_short_urls += each_val\n",
    "    short_url_count_dict[key] = total_short_urls\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250817"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find overall short url count \n",
    "total_short_url_count = 0\n",
    "\n",
    "for each_val in short_url_count_dict.values():\n",
    "    total_short_url_count += each_val\n",
    "total_short_url_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from urlparse import urlparse, urlsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "def get_number_of_clicks(page):\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    main_div = soup.find(id='main')\n",
    "    try:\n",
    "        clicks_data = main_div.find_all(class_='info-wrapper--clicks-text')\n",
    "        html = list(soup.children)[2]\n",
    "        body = list(html.children)[3]\n",
    "        script = list(body.children)[11].get_text()\n",
    "        str_script = unicodedata.normalize('NFKD', script).encode('ascii','ignore')\n",
    "        clicks = str_script.split(\"\\n\")[4].split(\":\")[1].split(\",\")[0]\n",
    "        return clicks\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_unshortened_url(link):\n",
    "    session = requests.Session()  # so connections are recycled\n",
    "    resp = session.head(url, allow_redirects=True)\n",
    "    return resp.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urlparse import urlparse, urlsplit\n",
    "def get_short_url_list(filename):\n",
    "    short_urls = []\n",
    "    page_df = pd.DataFrame()\n",
    "    page_df =  pandas.read_csv(filename)\n",
    "    for each_link in list(page_df[page_df['status_type']=='link']['status_link']):\n",
    "        try:\n",
    "            if urlparse.urlparse(each_link).netloc in url_shorteners:\n",
    "                short_urls.append(each_link) \n",
    "        except:\n",
    "            pass\n",
    "    return short_urls \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_url_page_ids = [str(x) for x in short_url_page_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_url_file_list = []\n",
    "for each_file in file_list:\n",
    "    page_id = each_file.split('/')[2].split('_')[0]\n",
    "    if page_id in short_url_page_ids:\n",
    "        short_url_file_list.append(each_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_url_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "def get_headers(short_url):\n",
    "    headers = {'method':'POST', 'path':'/proxy/v3/link/clicks',\n",
    "'accept':'*/*',\n",
    "'accept-language':'en-US,en;q=0.8',\n",
    "'alexatoolbar-alx_ns_ph':'AlexaToolbar/alx-4.0.1',\n",
    "'content-length':'110',\n",
    "'content-type':'application/x-www-form-urlencoded; charset=UTF-8',\n",
    "'cookie':'intercom-id-qma0svun=943ed489-02dd-445f-9c1c-04556f609c74; _bit=h7bfi3-2befb01299f4c3931f-00n; _cc=2; _xsrf=a4dac62b1d03413c93f92e76ad20f2b1; anon_u=cHN1X19mNzMwZjFkYi1lOTQ2LTQyYTUtOTMxMC00ZGQwMDk5YThkYzE=|1502842219|8566baf831d3131d7b28f6389a798d31e7b250f1; _ga=GA1.2.1089544842.1502034674; _gid=GA1.2.860537510.1502842117; user=MXxvXzRrYTlldHB2aGQ=|1502897110|47925a78b6054f7c02a5581e0cfe9747b6968ac6; intercom-session-qma0svun=Z0l2WHFPMlJNc3cwL2pGcThUTzg0UzdUL1BaUWM3U2lnZy9jTU00TzhDcUo5SnVDWDhyc2t1eGVKRGhvN3VrWS0td2VIRnp6cVNSUGtKSi9RUHdQVHZqUT09--49995d19fa7de6c226427428b7b6dac2d7b9f38d',\n",
    "'origin':'https://bitly.com',\n",
    "'referer': short_url + '+',\n",
    "'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "'x-bitly-client':'bbt2',\n",
    "'x-xsrftoken':'a4dac62b1d03413c93f92e76ad20f2b1'}\n",
    "    \n",
    "    params = {'link':short_url,\n",
    "    'timezone':0,\n",
    "    'unit':'month',\n",
    "    'units':22,\n",
    "    'rollup':'false'\n",
    "             }\n",
    "    return headers, params\n",
    "\n",
    "def get_clicks(short_url):\n",
    "    encoded_url = urllib.quote_plus(short_url)\n",
    "    bitly_url = 'https://api-ssl.bitly.com/v3/link/clicks?access_token=64a338894dd91bf6131b163fa5dc6aef284a2c52&link=%s&unit=day&units=-1&format=json'\n",
    "    req_url = requests.get(bitly_url%(encoded_url))\n",
    "    try:\n",
    "        data = req_url.json()\n",
    "        if data['status_code'] == 200:\n",
    "            return data\n",
    "        if data['status_code'] == 404:\n",
    "            return 'non_bitly'\n",
    "        if data['status_code'] == 403:\n",
    "            return 'forbidden'\n",
    "        return 'other'\n",
    "    except:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compress_and_save(resp, f_name, d_type='json'):\n",
    "    if d_type == 'json':\n",
    "        resp = json.dumps(resp)\n",
    "        f_name = f_name + '.json'\n",
    "    else:\n",
    "        f_name = f_name + '.txt.gz'\n",
    "#     resp = resp.encode('utf-8').encode('zlib_codec')\n",
    "    with open( f_name, 'w') as f:\n",
    "        f.write (resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/statuses/107705785934333_facebook_statuses.csv\n",
      "107705785934333\n",
      "193\n",
      "0\n",
      "data/statuses/108247159506719_facebook_statuses.csv\n",
      "108247159506719\n",
      "5949\n",
      "500\n",
      "data/statuses/108728919161573_facebook_statuses.csv\n",
      "108728919161573\n",
      "6\n",
      "500\n",
      "data/statuses/1089660871073245_facebook_statuses.csv\n",
      "1089660871073245\n",
      "4\n",
      "500\n",
      "data/statuses/112114435501106_facebook_statuses.csv\n",
      "112114435501106\n",
      "2\n",
      "500\n",
      "data/statuses/1125333984241125_facebook_statuses.csv\n",
      "1125333984241125\n",
      "8\n",
      "500\n",
      "data/statuses/112623813202_facebook_statuses.csv\n",
      "112623813202\n",
      "3885\n",
      "1000\n",
      "data/statuses/112723252096438_facebook_statuses.csv\n",
      "112723252096438\n",
      "35\n",
      "1000\n",
      "data/statuses/1129007383778921_facebook_statuses.csv\n",
      "1129007383778921\n",
      "12\n",
      "1000\n",
      "data/statuses/114345125931_facebook_statuses.csv\n",
      "114345125931\n",
      "18\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Find clicks count for each bitly URL\n",
    "\n",
    "import time\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "counter = 0\n",
    "sleep_time = 1.0\n",
    "for each_file in short_url_file_list:\n",
    "    page_id = each_file.split('/')[2].split('_')[0]\n",
    "    bitly_url_stats = {}\n",
    "    non_bitly_urls = []\n",
    "    short_urls = []\n",
    "    short_urls = get_short_url_list(each_file)\n",
    "    if len(set(short_urls)) != 0:\n",
    "        print len(short_urls)\n",
    "        for each_link in list(set(short_urls))[500:1000]:\n",
    "            clicks = get_clicks(each_link)\n",
    "            if clicks == 'non_bitly':\n",
    "                pass           \n",
    "            elif clicks == 'forbidden':\n",
    "                print each_link\n",
    "                sleep_time += 5.0 \n",
    "            elif clicks == 'other':\n",
    "                print each_link\n",
    "            else:\n",
    "                sleep_time = 0.5\n",
    "                path = '<path to directory>'+ page_id +\"/\"\n",
    "                if not os.path.exists(path):\n",
    "                    os.makedirs(path)\n",
    "                compress_and_save(clicks, path + each_link.split(\"/\")[2]+\"_\"+each_link.split(\"/\")[3])\n",
    "            counter += 1\n",
    "            if counter == 1000:\n",
    "                sleep_time += 5.0\n",
    "            time.sleep(sleep_time)  \n",
    "        if non_bitly_urls != []:\n",
    "            with open('non_bitly_short_urls/'+page_id+'_non_bitly.txt', 'w') as f:\n",
    "                f.write(json.dumps(non_bitly_urls, f))\n",
    "        print counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non_bitly_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if url_shorteners != []:\n",
    "    with open('url_shorteners.txt', 'w') as f:\n",
    "        f.write(json.dumps(url_shorteners))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
